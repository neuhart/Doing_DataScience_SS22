{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import geoplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_enc = pd.read_csv('data\\encounter-events.csv')\n",
    "df_loit = pd.read_csv('data\\loitering-events.csv')\n",
    "df_ves = pd.read_csv('data/transshipment-vessels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Encounter Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This code:\n",
    "\n",
    "* removes all entries with duplicate values over all features\n",
    "* removes outliers in duration_hr column\n",
    "* transforms timestamps into floats and creates new time features as datetime objects\n",
    "* adds a support feature to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11458 encounterings left in the dataset\n"
     ]
    }
   ],
   "source": [
    "df_enc_mod = df_enc.copy()\n",
    "\n",
    "df_enc_mod = df_enc_mod.loc[df_enc_mod['duration_hr'] <= 65]  # remove outliers detected in the EDA file\n",
    "\n",
    "df_enc_mod.drop_duplicates(inplace = True)  # remove duplicates\n",
    "\n",
    "df_enc_mod = df_enc_mod[df_enc_mod['fishing_vessel_mmsi'].astype(str).str.len() == 9]\n",
    "# remove all instances with wrong fishing_vessel_mmsi format\n",
    "\n",
    "print('{} encounterings left in the dataset'.format(df_enc_mod.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_enc_mod['start_time'] = pd.to_datetime(df_enc['start_time'])  # parse time objects\n",
    "df_enc_mod['end_time'] = pd.to_datetime(df_enc['end_time'])\n",
    "\n",
    "df_enc_mod['starting_timestamp'] = df_enc_mod['start_time'].apply(lambda x: x.timestamp()) # Changing format of timestamps from object to float\n",
    "df_enc_mod['ending_timestamp'] = df_enc_mod['end_time'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# df_enc_mod.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_enc_mod['from_encounter'] = 1  # support variable need later after merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#df_enc_mod.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loitering Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This code:\n",
    "\n",
    "* removes all entries with duplicate values over all features\n",
    "* remove outliers in total_even_duration column\n",
    "* Creates new feature and removes redundant features\n",
    "* transforms timestamps into floats and creates new time features as datetime objects\n",
    "* adds a support feature to the dataset\n",
    "* both transforms and renames features to those in the encounter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   transshipment_mmsi  starting_latitude  starting_longitude  ending_latitude  \\\n",
      "0           256064000          12.793800          -69.503235        12.727950   \n",
      "1           256064000          25.405013          -56.302800        25.393350   \n",
      "2           256064000          10.693413          -78.512800        10.484500   \n",
      "3           256064000          11.858933          -75.553383        12.007627   \n",
      "4           256064000          45.938747          -23.796960        45.976960   \n",
      "\n",
      "   ending_longitude    starting_timestamp      ending_timestamp  \\\n",
      "0        -69.739120  2017-09-06T02:11:32Z  2017-09-06T15:44:15Z   \n",
      "1        -56.488250  2017-07-22T13:19:49Z  2017-07-22T23:59:17Z   \n",
      "2        -78.884300  2017-11-29T14:48:51Z  2017-11-30T07:43:36Z   \n",
      "3        -75.535627  2017-09-07T23:39:03Z  2017-09-08T14:59:26Z   \n",
      "4        -23.449387  2017-08-17T23:18:21Z  2017-08-18T17:56:30Z   \n",
      "\n",
      "   median_speed_knots  total_event_duration  \n",
      "0            1.091065             13.915556  \n",
      "1            1.548335             10.897639  \n",
      "2            1.503456             18.001389  \n",
      "3            0.650355             16.193611  \n",
      "4            1.009874             19.748333  \n",
      "45919 loitering events left in the dataset\n"
     ]
    }
   ],
   "source": [
    "df_loit_mod =  df_loit.copy()\n",
    "print(df_loit_mod.head())\n",
    "df_loit_mod = df_loit_mod.loc[df_loit_mod['total_event_duration'] <= 200]  # remove outliers detected in the EDA file\n",
    "\n",
    "df_loit_mod.drop_duplicates(inplace = True)  # dropping duplicates\n",
    "\n",
    "print('{} loitering events left in the dataset'.format(df_loit_mod.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The next step consists of creating mean variable for both latitude and longitude coordinates to match the format of the encounter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_loit_mod['mean_latitude'] = (df_loit_mod['starting_latitude']+df_loit_mod['ending_latitude'])/2  # transforming locations to mean\n",
    "df_loit_mod['mean_longitude'] = (df_loit_mod['starting_longitude']+df_loit_mod['ending_longitude'])/2\n",
    "df_loit_mod.drop(columns=['starting_latitude','ending_latitude','starting_longitude', 'ending_longitude'], inplace=True)  # drop old columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we change the format of the dates columns to both a datetime and a timestamp (float) version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_loit_mod['start_time'] = pd.to_datetime(df_loit['starting_timestamp'])  # Create new start time column with datetime format\n",
    "df_loit_mod['end_time'] = pd.to_datetime(df_loit['ending_timestamp'])\n",
    "\n",
    "df_loit_mod['starting_timestamp'] = df_loit_mod['start_time'].apply(lambda x: x.timestamp()) # Changing format of timestamps from object to float\n",
    "df_loit_mod['ending_timestamp'] = df_loit_mod['end_time'].apply(lambda x: x.timestamp())\n",
    "\n",
    "# df_loit_mod.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We rename the transshipment column make clear that it's a transshipment VESSEL attribute not one of a possible transshipment itself\n",
    "and rename the duration column to match the format of the encounter data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_loit_mod.rename(columns ={'transshipment_mmsi':'transshipment_vessel_mmsi', 'total_event_duration':'duration_hr'}, inplace = True)\n",
    "\n",
    "df_loit_mod['from_encounter'] = 0  # support variable need later after merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#df_loit_mod.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we clip observations from the dataset that have a mean position on land\n",
    "\n",
    "This is caused by the transformation of the original features(starting and ending longitude/latitude) to the format in the encounter dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimensions: (45919, 11)\n",
      "After Clipping: (45894, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44996    POINT (177.79953 62.12609)\n",
       "17286    POINT (176.93786 62.13109)\n",
       "18421    POINT (176.99189 62.13220)\n",
       "28253    POINT (177.20941 62.13253)\n",
       "45094    POINT (177.65082 62.13685)\n",
       "                    ...            \n",
       "31928      POINT (-3.39932 3.83979)\n",
       "19516      POINT (-2.66728 3.84766)\n",
       "445        POINT (-5.76089 3.86324)\n",
       "43585      POINT (-5.70251 3.86888)\n",
       "12733      POINT (-7.19617 3.86951)\n",
       "Name: geometry, Length: 45894, dtype: geometry"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocean = gpd.read_file(\"external_data/ocean/ne_10m_ocean.shp\") #Importing Ocean Shapefile\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df_loit_mod['mean_longitude'], df_loit_mod['mean_latitude'])]\n",
    "gdf_loit_mod = gpd.GeoDataFrame(df_loit_mod.copy(), geometry=geometry) #Creating Geopandas Dataframe\n",
    "\n",
    "print(\"Original Dimensions: \"+str(gdf_loit_mod.shape))\n",
    "\n",
    "gdf_clip = gpd.clip(gdf_loit_mod,ocean) #Remove all observations on land \n",
    "\n",
    "print(\"After Clipping: \"+str(gdf_clip.shape))\n",
    "\n",
    "df_loit_mod = pd.DataFrame(gdf_clip) \n",
    "\n",
    "df_loit_mod.pop('geometry') # remove geometry objects from dataframe\n",
    "\n",
    "#df_loit_mod.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Vessel Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This code:\n",
    "\n",
    "* removes all entries with duplicate values in mmsi [Team Decision] to avoid issues with future joins over all datasets\n",
    "* removes all entries with NA or null values in the mmsi feature\n",
    "* transforms timestamps into datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_ves_mod = df_ves.copy()\n",
    "\n",
    "df_ves_mod.dropna(inplace=True)\n",
    "# this is necessary to avoid weird (non sql) behavior from merge (nulls are recognized as keys!)\n",
    "\n",
    "df_ves_mod = df_ves_mod[df_ves_mod['mmsi'].astype(int).astype(str).str.len() == 9]  # get rid of vessel with wrong mmsi format\n",
    "\n",
    "df_ves_mod.drop_duplicates(subset = ['mmsi'], inplace = True)  # dropping duplicates in feature mmsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For the dates, we want to have both times in datetime format and timestamps in float format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_ves_mod['first_time'] = pd.to_datetime(df_ves_mod['first_timestamp'])  # Create new first time column with datetime format\n",
    "df_ves_mod['last_time'] = pd.to_datetime(df_ves_mod['last_timestamp'])\n",
    "\n",
    "df_ves_mod['first_timestamp'] = df_ves_mod['first_time'].apply(lambda x: x.timestamp())  # Change format from object to float\n",
    "df_ves_mod['last_timestamp'] = df_ves_mod['last_time'].apply(lambda x: x.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df_ves_mod.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This code:\n",
    "\n",
    "* Aggregates(Stacks) Encounter and Loitering Datasets by keys and adds non matching features as additional columns\n",
    "* Reinserts the target feature as the first column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_agg = pd.concat([df_enc_mod,df_loit_mod], ignore_index=True)\n",
    "\n",
    "#setting support variable as first column:\n",
    "df_agg.insert(0, 'from_encounter',df_agg.pop('from_encounter'))\n",
    "\n",
    "# df_agg.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##  Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This code:\n",
    "\n",
    "* Merges the aggregated datasets with the vessel data on the key pair 'transshipment_vessel_mmsi' and 'mmsi'\n",
    "* This is sufficient because no fishing vessels are in the vessel dataset\n",
    "* manually adds a suffix of '_t' to all features originating from the vessel dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean_all = pd.merge(df_agg, df_ves_mod, how ='left', left_on ='transshipment_vessel_mmsi', right_on='mmsi')\n",
    "\n",
    "\n",
    "####Optional: add suffix to all added variables\n",
    "df_clean_all.rename(columns= { 'mmsi' : 'mmsi_t', 'shipname': 'shipname_t', 'callsign':'callsign_t', 'flag':'flag_t', 'imo': 'imo_t',\\\n",
    "     'first_timestamp': 'first_timestamp_t', 'last_timestamp':'last_timestamp_t'}, inplace = True)\n",
    "\n",
    "####This could be use to merge on fishing vessel, this is however not necessary in this context(No fishing vessels, only tranship)\n",
    "\n",
    "# df_clean_all = pd.merge(df_clean_all, df_ves_mod, how ='left', left_on ='fishing_vessel_mmsi', right_on='mmsi')\n",
    "# df_clean_all.rename(columns= { 'mmsi' : 'mmsi_f', 'shipname': 'shipname_f', 'callsign':'callsign_f', 'flag':'flag_f', 'imo': 'imo_f',\\\n",
    "#      'first_timestamp': 'first_timestamp_f', 'last_timestamp':'last_timestamp_f'}, inplace = True)\n",
    "\n",
    "df_clean_all.dropna(subset=['imo_t'],inplace=True) # some transshipment vessels apparently do not have an ímo code, those have to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 50379 entries, 0 to 57350\n",
      "Data columns (total 21 columns):\n",
      " #   Column                     Non-Null Count  Dtype              \n",
      "---  ------                     --------------  -----              \n",
      " 0   from_encounter             50379 non-null  int64              \n",
      " 1   fishing_vessel_mmsi        10126 non-null  float64            \n",
      " 2   transshipment_vessel_mmsi  50379 non-null  int64              \n",
      " 3   start_time                 50379 non-null  datetime64[ns, UTC]\n",
      " 4   end_time                   50379 non-null  datetime64[ns, UTC]\n",
      " 5   mean_latitude              50379 non-null  float64            \n",
      " 6   mean_longitude             50379 non-null  float64            \n",
      " 7   duration_hr                50379 non-null  float64            \n",
      " 8   median_distance_km         10126 non-null  float64            \n",
      " 9   median_speed_knots         50379 non-null  float64            \n",
      " 10  starting_timestamp         50379 non-null  float64            \n",
      " 11  ending_timestamp           50379 non-null  float64            \n",
      " 12  mmsi_t                     50379 non-null  float64            \n",
      " 13  shipname_t                 50379 non-null  object             \n",
      " 14  callsign_t                 50379 non-null  object             \n",
      " 15  flag_t                     50379 non-null  object             \n",
      " 16  imo_t                      50379 non-null  float64            \n",
      " 17  first_timestamp_t          50379 non-null  float64            \n",
      " 18  last_timestamp_t           50379 non-null  float64            \n",
      " 19  first_time                 50379 non-null  datetime64[ns, UTC]\n",
      " 20  last_time                  50379 non-null  datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](4), float64(12), int64(2), object(3)\n",
      "memory usage: 8.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_clean_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from_encounter</th>\n",
       "      <th>fishing_vessel_mmsi</th>\n",
       "      <th>transshipment_vessel_mmsi</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>mean_latitude</th>\n",
       "      <th>mean_longitude</th>\n",
       "      <th>duration_hr</th>\n",
       "      <th>median_distance_km</th>\n",
       "      <th>median_speed_knots</th>\n",
       "      <th>...</th>\n",
       "      <th>ending_timestamp</th>\n",
       "      <th>mmsi_t</th>\n",
       "      <th>shipname_t</th>\n",
       "      <th>callsign_t</th>\n",
       "      <th>flag_t</th>\n",
       "      <th>imo_t</th>\n",
       "      <th>first_timestamp_t</th>\n",
       "      <th>last_timestamp_t</th>\n",
       "      <th>first_time</th>\n",
       "      <th>last_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>416565000.0</td>\n",
       "      <td>354240000</td>\n",
       "      <td>2016-11-18 14:30:00+00:00</td>\n",
       "      <td>2016-11-19 01:50:00+00:00</td>\n",
       "      <td>-17.039085</td>\n",
       "      <td>-79.063725</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>0.038188</td>\n",
       "      <td>0.585402</td>\n",
       "      <td>...</td>\n",
       "      <td>1.479520e+09</td>\n",
       "      <td>354240000.0</td>\n",
       "      <td>CRYSTAL REEFER</td>\n",
       "      <td>3FFY5</td>\n",
       "      <td>PAN</td>\n",
       "      <td>9017276.0</td>\n",
       "      <td>1.422748e+09</td>\n",
       "      <td>1.489109e+09</td>\n",
       "      <td>2015-01-31 23:43:15+00:00</td>\n",
       "      <td>2017-03-10 01:21:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>412679190.0</td>\n",
       "      <td>354240000</td>\n",
       "      <td>2016-12-11 14:50:00+00:00</td>\n",
       "      <td>2016-12-11 19:50:00+00:00</td>\n",
       "      <td>-20.269608</td>\n",
       "      <td>-79.244953</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.020033</td>\n",
       "      <td>0.575663</td>\n",
       "      <td>...</td>\n",
       "      <td>1.481486e+09</td>\n",
       "      <td>354240000.0</td>\n",
       "      <td>CRYSTAL REEFER</td>\n",
       "      <td>3FFY5</td>\n",
       "      <td>PAN</td>\n",
       "      <td>9017276.0</td>\n",
       "      <td>1.422748e+09</td>\n",
       "      <td>1.489109e+09</td>\n",
       "      <td>2015-01-31 23:43:15+00:00</td>\n",
       "      <td>2017-03-10 01:21:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>440863000.0</td>\n",
       "      <td>354240000</td>\n",
       "      <td>2017-06-13 12:50:00+00:00</td>\n",
       "      <td>2017-06-15 01:20:00+00:00</td>\n",
       "      <td>-62.640767</td>\n",
       "      <td>-60.690240</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>0.054992</td>\n",
       "      <td>0.019775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.497490e+09</td>\n",
       "      <td>354240000.0</td>\n",
       "      <td>CRYSTAL REEFER</td>\n",
       "      <td>3FFY5</td>\n",
       "      <td>PAN</td>\n",
       "      <td>9017276.0</td>\n",
       "      <td>1.422748e+09</td>\n",
       "      <td>1.489109e+09</td>\n",
       "      <td>2015-01-31 23:43:15+00:00</td>\n",
       "      <td>2017-03-10 01:21:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>416563000.0</td>\n",
       "      <td>354240000</td>\n",
       "      <td>2016-11-15 11:30:00+00:00</td>\n",
       "      <td>2016-11-16 04:00:00+00:00</td>\n",
       "      <td>-17.046586</td>\n",
       "      <td>-79.061923</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>0.036427</td>\n",
       "      <td>1.023917</td>\n",
       "      <td>...</td>\n",
       "      <td>1.479269e+09</td>\n",
       "      <td>354240000.0</td>\n",
       "      <td>CRYSTAL REEFER</td>\n",
       "      <td>3FFY5</td>\n",
       "      <td>PAN</td>\n",
       "      <td>9017276.0</td>\n",
       "      <td>1.422748e+09</td>\n",
       "      <td>1.489109e+09</td>\n",
       "      <td>2015-01-31 23:43:15+00:00</td>\n",
       "      <td>2017-03-10 01:21:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>441309000.0</td>\n",
       "      <td>354240000</td>\n",
       "      <td>2017-05-19 00:40:00+00:00</td>\n",
       "      <td>2017-05-19 20:50:00+00:00</td>\n",
       "      <td>-46.627878</td>\n",
       "      <td>-60.554922</td>\n",
       "      <td>20.166667</td>\n",
       "      <td>0.034053</td>\n",
       "      <td>0.544031</td>\n",
       "      <td>...</td>\n",
       "      <td>1.495227e+09</td>\n",
       "      <td>354240000.0</td>\n",
       "      <td>CRYSTAL REEFER</td>\n",
       "      <td>3FFY5</td>\n",
       "      <td>PAN</td>\n",
       "      <td>9017276.0</td>\n",
       "      <td>1.422748e+09</td>\n",
       "      <td>1.489109e+09</td>\n",
       "      <td>2015-01-31 23:43:15+00:00</td>\n",
       "      <td>2017-03-10 01:21:55+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57346</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>306862000</td>\n",
       "      <td>2015-07-29 10:20:41+00:00</td>\n",
       "      <td>2015-07-30 08:45:32+00:00</td>\n",
       "      <td>3.835017</td>\n",
       "      <td>-2.807666</td>\n",
       "      <td>22.965694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.320632</td>\n",
       "      <td>...</td>\n",
       "      <td>1.438246e+09</td>\n",
       "      <td>306862000.0</td>\n",
       "      <td>COPPENAME</td>\n",
       "      <td>PJYM</td>\n",
       "      <td>CUW</td>\n",
       "      <td>8807636.0</td>\n",
       "      <td>1.325379e+09</td>\n",
       "      <td>1.530394e+09</td>\n",
       "      <td>2012-01-01 00:52:31+00:00</td>\n",
       "      <td>2018-06-30 21:26:04+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57347</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>306862000</td>\n",
       "      <td>2015-07-27 21:02:41+00:00</td>\n",
       "      <td>2015-07-28 10:58:54+00:00</td>\n",
       "      <td>3.839788</td>\n",
       "      <td>-3.399318</td>\n",
       "      <td>15.129028</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.455701</td>\n",
       "      <td>...</td>\n",
       "      <td>1.438081e+09</td>\n",
       "      <td>306862000.0</td>\n",
       "      <td>COPPENAME</td>\n",
       "      <td>PJYM</td>\n",
       "      <td>CUW</td>\n",
       "      <td>8807636.0</td>\n",
       "      <td>1.325379e+09</td>\n",
       "      <td>1.530394e+09</td>\n",
       "      <td>2012-01-01 00:52:31+00:00</td>\n",
       "      <td>2018-06-30 21:26:04+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57348</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>616401000</td>\n",
       "      <td>2013-03-03 08:07:11+00:00</td>\n",
       "      <td>2013-03-06 12:58:29+00:00</td>\n",
       "      <td>3.847658</td>\n",
       "      <td>-2.667285</td>\n",
       "      <td>78.762083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.096260</td>\n",
       "      <td>...</td>\n",
       "      <td>1.362575e+09</td>\n",
       "      <td>616401000.0</td>\n",
       "      <td>SILVER ICE</td>\n",
       "      <td>D6DA5</td>\n",
       "      <td>COM</td>\n",
       "      <td>7819759.0</td>\n",
       "      <td>1.327360e+09</td>\n",
       "      <td>1.530398e+09</td>\n",
       "      <td>2012-01-23 23:00:01+00:00</td>\n",
       "      <td>2018-06-30 22:31:53+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57349</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>306752000</td>\n",
       "      <td>2015-09-13 16:24:39+00:00</td>\n",
       "      <td>2015-09-14 03:42:46+00:00</td>\n",
       "      <td>3.863237</td>\n",
       "      <td>-5.760889</td>\n",
       "      <td>16.503472</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.970459</td>\n",
       "      <td>...</td>\n",
       "      <td>1.442202e+09</td>\n",
       "      <td>306752000.0</td>\n",
       "      <td>NOVA FLORIDA</td>\n",
       "      <td>PJUP</td>\n",
       "      <td>CUW</td>\n",
       "      <td>8813635.0</td>\n",
       "      <td>1.325543e+09</td>\n",
       "      <td>1.530391e+09</td>\n",
       "      <td>2012-01-02 22:19:40+00:00</td>\n",
       "      <td>2018-06-30 20:29:17+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57350</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>306497000</td>\n",
       "      <td>2016-04-04 11:01:20+00:00</td>\n",
       "      <td>2016-04-05 15:41:29+00:00</td>\n",
       "      <td>3.868877</td>\n",
       "      <td>-5.702513</td>\n",
       "      <td>28.987222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.561448</td>\n",
       "      <td>...</td>\n",
       "      <td>1.459871e+09</td>\n",
       "      <td>306497000.0</td>\n",
       "      <td>NOVA ZEELANDIA</td>\n",
       "      <td>PJHA</td>\n",
       "      <td>CUW</td>\n",
       "      <td>8514784.0</td>\n",
       "      <td>1.325754e+09</td>\n",
       "      <td>1.530402e+09</td>\n",
       "      <td>2012-01-05 09:06:17+00:00</td>\n",
       "      <td>2018-06-30 23:44:34+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50379 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       from_encounter  fishing_vessel_mmsi  transshipment_vessel_mmsi  \\\n",
       "0                   1          416565000.0                  354240000   \n",
       "1                   1          412679190.0                  354240000   \n",
       "2                   1          440863000.0                  354240000   \n",
       "3                   1          416563000.0                  354240000   \n",
       "4                   1          441309000.0                  354240000   \n",
       "...               ...                  ...                        ...   \n",
       "57346               0                  NaN                  306862000   \n",
       "57347               0                  NaN                  306862000   \n",
       "57348               0                  NaN                  616401000   \n",
       "57349               0                  NaN                  306752000   \n",
       "57350               0                  NaN                  306497000   \n",
       "\n",
       "                     start_time                  end_time  mean_latitude  \\\n",
       "0     2016-11-18 14:30:00+00:00 2016-11-19 01:50:00+00:00     -17.039085   \n",
       "1     2016-12-11 14:50:00+00:00 2016-12-11 19:50:00+00:00     -20.269608   \n",
       "2     2017-06-13 12:50:00+00:00 2017-06-15 01:20:00+00:00     -62.640767   \n",
       "3     2016-11-15 11:30:00+00:00 2016-11-16 04:00:00+00:00     -17.046586   \n",
       "4     2017-05-19 00:40:00+00:00 2017-05-19 20:50:00+00:00     -46.627878   \n",
       "...                         ...                       ...            ...   \n",
       "57346 2015-07-29 10:20:41+00:00 2015-07-30 08:45:32+00:00       3.835017   \n",
       "57347 2015-07-27 21:02:41+00:00 2015-07-28 10:58:54+00:00       3.839788   \n",
       "57348 2013-03-03 08:07:11+00:00 2013-03-06 12:58:29+00:00       3.847658   \n",
       "57349 2015-09-13 16:24:39+00:00 2015-09-14 03:42:46+00:00       3.863237   \n",
       "57350 2016-04-04 11:01:20+00:00 2016-04-05 15:41:29+00:00       3.868877   \n",
       "\n",
       "       mean_longitude  duration_hr  median_distance_km  median_speed_knots  \\\n",
       "0          -79.063725    11.333333            0.038188            0.585402   \n",
       "1          -79.244953     5.000000            0.020033            0.575663   \n",
       "2          -60.690240    36.500000            0.054992            0.019775   \n",
       "3          -79.061923    16.500000            0.036427            1.023917   \n",
       "4          -60.554922    20.166667            0.034053            0.544031   \n",
       "...               ...          ...                 ...                 ...   \n",
       "57346       -2.807666    22.965694                 NaN            1.320632   \n",
       "57347       -3.399318    15.129028                 NaN            1.455701   \n",
       "57348       -2.667285    78.762083                 NaN            1.096260   \n",
       "57349       -5.760889    16.503472                 NaN            1.970459   \n",
       "57350       -5.702513    28.987222                 NaN            1.561448   \n",
       "\n",
       "       ...  ending_timestamp       mmsi_t      shipname_t callsign_t flag_t  \\\n",
       "0      ...      1.479520e+09  354240000.0  CRYSTAL REEFER      3FFY5    PAN   \n",
       "1      ...      1.481486e+09  354240000.0  CRYSTAL REEFER      3FFY5    PAN   \n",
       "2      ...      1.497490e+09  354240000.0  CRYSTAL REEFER      3FFY5    PAN   \n",
       "3      ...      1.479269e+09  354240000.0  CRYSTAL REEFER      3FFY5    PAN   \n",
       "4      ...      1.495227e+09  354240000.0  CRYSTAL REEFER      3FFY5    PAN   \n",
       "...    ...               ...          ...             ...        ...    ...   \n",
       "57346  ...      1.438246e+09  306862000.0       COPPENAME       PJYM    CUW   \n",
       "57347  ...      1.438081e+09  306862000.0       COPPENAME       PJYM    CUW   \n",
       "57348  ...      1.362575e+09  616401000.0      SILVER ICE      D6DA5    COM   \n",
       "57349  ...      1.442202e+09  306752000.0    NOVA FLORIDA       PJUP    CUW   \n",
       "57350  ...      1.459871e+09  306497000.0  NOVA ZEELANDIA       PJHA    CUW   \n",
       "\n",
       "           imo_t  first_timestamp_t  last_timestamp_t  \\\n",
       "0      9017276.0       1.422748e+09      1.489109e+09   \n",
       "1      9017276.0       1.422748e+09      1.489109e+09   \n",
       "2      9017276.0       1.422748e+09      1.489109e+09   \n",
       "3      9017276.0       1.422748e+09      1.489109e+09   \n",
       "4      9017276.0       1.422748e+09      1.489109e+09   \n",
       "...          ...                ...               ...   \n",
       "57346  8807636.0       1.325379e+09      1.530394e+09   \n",
       "57347  8807636.0       1.325379e+09      1.530394e+09   \n",
       "57348  7819759.0       1.327360e+09      1.530398e+09   \n",
       "57349  8813635.0       1.325543e+09      1.530391e+09   \n",
       "57350  8514784.0       1.325754e+09      1.530402e+09   \n",
       "\n",
       "                     first_time                 last_time  \n",
       "0     2015-01-31 23:43:15+00:00 2017-03-10 01:21:55+00:00  \n",
       "1     2015-01-31 23:43:15+00:00 2017-03-10 01:21:55+00:00  \n",
       "2     2015-01-31 23:43:15+00:00 2017-03-10 01:21:55+00:00  \n",
       "3     2015-01-31 23:43:15+00:00 2017-03-10 01:21:55+00:00  \n",
       "4     2015-01-31 23:43:15+00:00 2017-03-10 01:21:55+00:00  \n",
       "...                         ...                       ...  \n",
       "57346 2012-01-01 00:52:31+00:00 2018-06-30 21:26:04+00:00  \n",
       "57347 2012-01-01 00:52:31+00:00 2018-06-30 21:26:04+00:00  \n",
       "57348 2012-01-23 23:00:01+00:00 2018-06-30 22:31:53+00:00  \n",
       "57349 2012-01-02 22:19:40+00:00 2018-06-30 20:29:17+00:00  \n",
       "57350 2012-01-05 09:06:17+00:00 2018-06-30 23:44:34+00:00  \n",
       "\n",
       "[50379 rows x 21 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Calculating Distance to Coast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Code:\n",
    "\n",
    "* Calculates the distance to the next coast or island and adds it as an additional feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometry = [Point(xy) for xy in zip(df_clean_all['mean_longitude'], df_clean_all['mean_latitude'])]\n",
    "gdf_clean_all = gpd.GeoDataFrame(df_clean_all.copy(), geometry=geometry)  \n",
    "world = gpd.read_file('external_data/earth/ne_10m_admin_0_countries.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        POINT (-79.06372 -17.03909)\n",
       "1        POINT (-79.24495 -20.26961)\n",
       "2        POINT (-60.69024 -62.64077)\n",
       "3        POINT (-79.06192 -17.04659)\n",
       "4        POINT (-60.55492 -46.62788)\n",
       "                    ...             \n",
       "57346       POINT (-2.80767 3.83502)\n",
       "57347       POINT (-3.39932 3.83979)\n",
       "57348       POINT (-2.66728 3.84766)\n",
       "57349       POINT (-5.76089 3.86324)\n",
       "57350       POINT (-5.70251 3.86888)\n",
       "Name: geometry, Length: 50379, dtype: geometry"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_clean_all = gdf_clean_all.set_crs('epsg:4326')\n",
    "\n",
    "#because we want to calculate our distances in meters we need to project our geometries to metric crs\n",
    "##https://epsg.io/3395 \n",
    "gdf_clean_all_temp = gdf_clean_all.to_crs(epsg=3395)\n",
    "\n",
    "#Creating 3395 land mass\n",
    "\n",
    "world_merc = world.set_crs('epsg:4326')\n",
    "\n",
    "#drop antarctica:\n",
    "#world_merc = world[(world.name != \"Antarctica\")] #dropping Antarctica, requires importing additional features from the NE dataset\n",
    "\n",
    "#Project to mercator\n",
    "\n",
    "world_merc = world_merc.to_crs(\"EPSG:3395\")\n",
    "\n",
    "#attempt to simplify, by buffering borders and therefore also coastlines by 1000 m\n",
    "#This is necesarry to create valid geometries for GeoDataFrame.unary_union to handle\n",
    "\n",
    "gdf_clean_all['distance_coast'] = gdf_clean_all_temp.distance(world_merc.buffer(1000).unary_union)/1000 ###/1000 to change scale to km\n",
    "\n",
    "\n",
    "df_clean_all = pd.DataFrame(gdf_clean_all) \n",
    "\n",
    "df_clean_all.pop('geometry') # remove geometry objects from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Output\n",
    "Now we are saving the dataset that we are going to use for clustering in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_clean_all.to_csv('data/cleaned_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d009efec00293f9fadca53c9b01bbe3de4453eb1c43fe096751e21fab7075340"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
